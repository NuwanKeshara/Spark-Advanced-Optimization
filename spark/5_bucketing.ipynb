{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_file = \"../data/bucketing/orders.csv\"\n",
    "df_orders = spark.read.csv(orders_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date|total_amount|\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|1       |80        |10         |4       |2023-3-20 |1003        |\n",
      "|2       |69        |30         |3       |2023-12-11|780         |\n",
      "|3       |61        |20         |4       |2023-4-26 |1218        |\n",
      "|4       |62        |44         |3       |2023-8-26 |2022        |\n",
      "|5       |78        |46         |4       |2023-8-5  |1291        |\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5, False)\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_file = \"../data/bucketing/products.csv\"\n",
    "df_products = spark.read.csv(products_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|product_id|product_name|category   |brand  |price|stock|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|1         |Product_1   |Electronics|Brand_4|26   |505  |\n",
      "|2         |Product_2   |Apparel    |Brand_4|489  |15   |\n",
      "|3         |Product_3   |Apparel    |Brand_4|102  |370  |\n",
      "|4         |Product_4   |Groceries  |Brand_1|47   |433  |\n",
      "|5         |Product_5   |Groceries  |Brand_3|244  |902  |\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show(5, False)\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.select(\"product_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.select(\"order_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing In Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=\"product_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#245, order_id#244, customer_id#246, quantity#247, order_date#248, total_amount#249, product_name#304, category#305, brand#306, price#307, stock#308]\n",
      "   +- SortMergeJoin [product_id#245], [product_id#303], Inner\n",
      "      :- Sort [product_id#245 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(product_id#245, 200), ENSURE_REQUIREMENTS, [id=#760]\n",
      "      :     +- Filter isnotnull(product_id#245)\n",
      "      :        +- FileScan csv [order_id#244,product_id#245,customer_id#246,quantity#247,order_date#248,total_amount#249] Batched: false, DataFilters: [isnotnull(product_id#245)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:string,total_amount:int>\n",
      "      +- Sort [product_id#303 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(product_id#303, 200), ENSURE_REQUIREMENTS, [id=#761]\n",
      "            +- Filter isnotnull(product_id#303)\n",
      "               +- FileScan csv [product_id#303,product_name#304,category#305,brand#306,price#307,stock#308] Batched: false, DataFilters: [isnotnull(product_id#303)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_products\n",
    "    .write.bucketBy(4, col=\"product_id\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"products_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_orders\n",
    "    .write.bucketBy(4, col=\"product_id\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"orders_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bucketed = spark.table(\"orders_bucketed\")\n",
    "df_products_bucketed = spark.table(\"products_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details_bucketed = (\n",
    "    df_orders_bucketed.join(\n",
    "        df_products_bucketed,\n",
    "        on=\"product_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#416, order_id#415, customer_id#417, quantity#418, order_date#419, total_amount#420, product_name#428, category#429, brand#430, price#431, stock#432]\n",
      "   +- SortMergeJoin [product_id#416], [product_id#427], Inner\n",
      "      :- Sort [product_id#416 ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(product_id#416)\n",
      "      :     +- FileScan parquet default.orders_bucketed[order_id#415,product_id#416,customer_id#417,quantity#418,order_date#419,total_amount#420] Batched: true, DataFilters: [isnotnull(product_id#416)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:string,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "      +- Sort [product_id#427 ASC NULLS FIRST], false, 0\n",
      "         +- Filter isnotnull(product_id#427)\n",
      "            +- FileScan parquet default.products_bucketed[product_id#427,product_name#428,category#429,brand#430,price#431,stock#432] Batched: true, DataFilters: [isnotnull(product_id#427)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing In Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date|total_amount|\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|1       |80        |10         |4       |2023-3-20 |1003        |\n",
      "|2       |69        |30         |3       |2023-12-11|780         |\n",
      "|3       |61        |20         |4       |2023-4-26 |1218        |\n",
      "|4       |62        |44         |3       |2023-8-26 |2022        |\n",
      "|5       |78        |46         |4       |2023-8-5  |1291        |\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_sales = (\n",
    "    df_orders\n",
    "    .filter(F.col(\"order_date\") == '2023-12-11')\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#245], functions=[sum(total_amount#249)])\n",
      "   +- Exchange hashpartitioning(product_id#245, 200), ENSURE_REQUIREMENTS, [id=#1155]\n",
      "      +- HashAggregate(keys=[product_id#245], functions=[partial_sum(total_amount#249)])\n",
      "         +- Project [product_id#245, total_amount#249]\n",
      "            +- Filter (isnotnull(order_date#248) AND (order_date#248 = 2023-12-11))\n",
      "               +- FileScan csv [product_id#245,order_date#248,total_amount#249] Batched: false, DataFilters: [isnotnull(order_date#248), (order_date#248 = 2023-12-11)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(order_date), EqualTo(order_date,2023-12-11)], ReadSchema: struct<product_id:int,order_date:string,total_amount:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sales.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_orders\n",
    "    .write.partitionBy(\"order_date\")\n",
    "    .bucketBy(4, \"product_id\")\n",
    "    .saveAsTable(\"orders_2_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bucketed_2 = spark.read.table(\"orders_2_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_sales_bucketed = (\n",
    "    df_orders_bucketed_2\n",
    "    .filter(F.col(\"order_date\") == '2023-12-11')\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#535], functions=[sum(total_amount#538)])\n",
      "   +- HashAggregate(keys=[product_id#535], functions=[partial_sum(total_amount#538)])\n",
      "      +- Project [product_id#535, total_amount#538]\n",
      "         +- FileScan parquet default.orders_2_bucketed[product_id#535,total_amount#538,order_date#539] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [isnotnull(order_date#539), (order_date#539 = 2023-12-11)], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sales_bucketed.explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
