{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/17 22:34:32 WARN Utils: Your hostname, Afaques-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.6 instead (on interface en0)\n",
      "23/07/17 22:34:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/17 22:34:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"18g\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_file = \"../data/data_skew/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city       |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|10   |7  |Entertainment|10.42 |boston     |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|3    |27 |Motor/Travel |44.34 |portland   |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|4    |11 |Entertainment|3.18  |chicago    |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|2    |22 |Groceries    |268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|10   |16 |Entertainment|2.66  |chicago    |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = \"../data/data_skew/customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Even though the DataFrame shows `12` partitions with `df_transactions.rdd.getNumPartitions()`, the Parquet file is small enough to be read as a single split. Therefore, Spark shows only 1 task for the read operation in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+------+---------+\n",
      "|cust_id   |first_name|last_name|age |gender|birthday |\n",
      "+----------+----------+---------+----+------+---------+\n",
      "|C007YEYTX9|Aaron     |Abbott   |39.0|Female|7/13/1991|\n",
      "|C08XAQUY73|Aaron     |Lambert  |59.0|Female|11/5/1966|\n",
      "|C094P1VXF9|Aaron     |Lindsey  |29.0|Male  |9/21/1990|\n",
      "|C097SHE1EF|Aaron     |Lopez    |27.0|Female|4/18/2001|\n",
      "|C0DTC6436T|Aaron     |Schwartz |57.0|Female|7/9/1962 |\n",
      "|C0R42FPHRH|Abbie     |Reyes    |68.0|Male  |10/8/1995|\n",
      "|C0RZV4BH7T|Abbie     |Stevenson|41.0|Male  |2/10/1971|\n",
      "+----------+----------+---------+----+------+---------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transform = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"city\") == \"boston\")\n",
    "    .withColumn(\"first_name\", F.split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", F.split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", F.col(\"age\") + F.lit(5))\n",
    "    .select(\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\")\n",
    ")\n",
    "\n",
    "df_narrow_transform.show(7, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|cust_id   |name          |age|gender|birthday  |zip  |city        |\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|C01BKUFRHA|Aaron Becker  |54 |Male  |11/24/1979|40284|san_diego   |\n",
      "|C01WMZQ7PN|Aaron Brady   |51 |Female|8/20/1994 |52204|philadelphia|\n",
      "|C021567NJZ|Aaron Briggs  |57 |Male  |3/10/1990 |22008|philadelphia|\n",
      "|C02JNTM46B|Aaron Chambers|51 |Male  |1/6/2001  |63337|new_york    |\n",
      "|C030A69V1L|Aaron Clarke  |55 |Male  |4/28/1999 |77176|philadelphia|\n",
      "|C034RB2MQ6|Aaron Ford    |63 |Male  |7/8/1988  |90592|chicago     |\n",
      "|C03U340T3R|Aaron Gardner |59 |Female|3/18/1975 |31502|denver      |\n",
      "|C044XUK8IK|Aaron Gibbs   |58 |Male  |5/17/1999 |67547|seattle     |\n",
      "|C0694GX5HW|Aaron Guerrero|62 |Male  |8/17/1976 |39243|denver      |\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer_gt_50 = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"age\").cast(\"int\") > 50)\n",
    ")\n",
    "df_customer_gt_50.show(9, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Transformations\n",
    "1. Repartition\n",
    "2. Coalesce\n",
    "3. Joins\n",
    "4. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned = df_transactions.repartition(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===========================================>             (10 + 3) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city         |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+\n",
      "|C0CZCC9UCH|2011-12-01|2019-03-01|TTU5VP07BG1OO6Y|2015-09-17|2015|9    |17 |Education    |49.76 |los_angeles  |\n",
      "|C0YDPQWPBJ|2011-11-01|null      |TKQ16P00JYR4D4R|2017-03-06|2017|3    |6  |Entertainment|147.19|denver       |\n",
      "|CHPV658EGQ|2012-03-01|2020-06-01|THIDSLZ2NRS622G|2017-01-11|2017|1    |11 |Entertainment|12.01 |los_angeles  |\n",
      "|C0W0YS75TQ|2011-03-01|2019-06-01|TX2MW4UTN1DIMQ4|2013-09-08|2013|9    |8  |Motor/Travel |39.18 |portland     |\n",
      "|C0YDPQWPBJ|2010-12-01|2019-07-01|TZRGI6BM0CTAC2V|2016-09-07|2016|9    |7  |Groceries    |367.73|san_francisco|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartitioned.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 5\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(5).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=========================================================(13 + 0) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "|cust_id   |start_date|end_date|txn_id         |date      |year|month|day|expense_type |amt    |city         |name        |age|gender|birthday |zip  |city  |\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "|C00WRSJF1Q|2012-11-01|null    |TXNU40MYVB3QXBU|2018-11-01|2018|11   |1  |Motor/Travel |2129.82|san_diego    |Aaron Barnes|29 |Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|null    |TKGK0XNNTDI0MPX|2014-08-06|2014|8    |6  |Groceries    |126.65 |boston       |Aaron Barnes|29 |Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|null    |T1QLRMJWEYOP8YD|2015-09-10|2015|9    |10 |Entertainment|28.94  |new_york     |Aaron Barnes|29 |Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|null    |T7YCEUYHV6FCVRR|2020-06-25|2020|6    |25 |Education    |21.34  |san_diego    |Aaron Barnes|29 |Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|null    |T5QYMWG3TDXF7HK|2018-12-26|2018|12   |26 |Entertainment|22.43  |san_francisco|Aaron Barnes|29 |Female|3/11/1977|23451|denver|\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|city     |count  |\n",
      "+---------+-------+\n",
      "|san_diego|3977780|\n",
      "|chicago  |3979023|\n",
      "|denver   |3980274|\n",
      "|boston   |3978268|\n",
      "|seattle  |3980022|\n",
      "+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_city_counts.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count Distinct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.countDistinct(\"txn_id\").alias(\"txn_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|city     |txn_count|\n",
      "+---------+---------+\n",
      "|san_diego|3977780  |\n",
      "|chicago  |3979023  |\n",
      "|denver   |3980274  |\n",
      "|boston   |3978268  |\n",
      "|seattle  |3980022  |\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_txn_per_city.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.sum(\"amt\").alias(\"txn_amt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, sum('amt) AS txn_amt#399]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, txn_amt: double\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#399]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#399]\n",
      "+- Project [amt#9, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[sum(cast(amt#9 as double))], output=[city#10, txn_amt#399])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [id=#416]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_sum(cast(amt#9 as double))], output=[city#10, sum#403])\n",
      "         +- FileScan parquet [amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<amt:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txn_amt_city.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
