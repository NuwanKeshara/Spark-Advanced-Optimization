{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Imports & Configuration </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 14:41:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25578ece-41dd-49fa-807e-7b5798d875d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Simulating Skewed Join </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2134bcf-2fd7-41c9-9fc1-32328c196482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|1    |\n",
      "|2    |\n",
      "|3    |\n",
      "|4    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 20:05:34 WARN TaskSetManager: Stage 55 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "df_uniform = spark.createDataFrame([i for i in range(1000000)], IntegerType())\n",
    "df_uniform.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a6b2e9-3543-4f5d-82ee-3f7e33139318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 20:04:29 WARN TaskSetManager: Stage 53 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|749568|\n",
      "|        1|749568|\n",
      "|        2|749568|\n",
      "|        3|751296|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uniform.withColumn(\n",
    "    \"partition\", F.spark_partition_id()\n",
    ").groupBy(\"partition\").count().orderBy(\"partition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0babff23-9b27-439a-80b2-a424dc6733f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = spark.createDataFrame([0] * 999990, IntegerType()).repartition(1)\n",
    "df1 = spark.createDataFrame([1] * 5, IntegerType()).repartition(1)\n",
    "df2 = spark.createDataFrame([2] * 5, IntegerType()).repartition(1)\n",
    "df_skew = df0.union(df1).union(df2)\n",
    "df_skew.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a33df63-459d-4bb3-88d2-5189334be1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|999990|\n",
      "|        1|     5|\n",
      "|        2|     5|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew.withColumn(\n",
    "    \"partition\", F.spark_partition_id()\n",
    ").groupBy(\"partition\").count().orderBy(\"partition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c184c2d-241f-4aff-b59b-825579fea717",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 14:41:47 WARN TaskSetManager: Stage 14 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|249856|\n",
      "|        2|249856|\n",
      "|        3|250432|\n",
      "|        1|249856|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uniform.withColumn(\"partition\", F.spark_partition_id()).groupBy(\"partition\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75213785-809b-492c-a0ff-5a775c36bfd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined_c1 = df_skew.join(df_uniform, \"value\", 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60ef213-f51c-4178-af15-92e7fc020bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 14:41:48 WARN TaskSetManager: Stage 21 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 20:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|999995|\n",
      "|        1|     5|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined_c1.withColumn(\"partition\", F.spark_partition_id()).groupBy(\"partition\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5156d1a2-2f8b-4391-af89-bfb119fd5dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Simulating Uniform Distribution Through Salting </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f90516c-fd1d-431e-ab98-f8030377d4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SALT_NUMBER = 3\n",
    "# SALT_NUMBER = spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8ee313-e419-425f-9e44-dc9ad0370d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew = df_skew.withColumn(\"salt\", (F.rand() * SALT_NUMBER).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3dbd79-a96d-4b0d-a97e-245d401b438a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|value|salt|\n",
      "+-----+----+\n",
      "|0    |0   |\n",
      "|0    |0   |\n",
      "|0    |2   |\n",
      "|0    |2   |\n",
      "|0    |1   |\n",
      "|0    |0   |\n",
      "|0    |1   |\n",
      "|0    |2   |\n",
      "|0    |1   |\n",
      "|0    |1   |\n",
      "|0    |1   |\n",
      "|0    |1   |\n",
      "|0    |0   |\n",
      "|0    |2   |\n",
      "|0    |2   |\n",
      "|0    |1   |\n",
      "|0    |0   |\n",
      "|0    |1   |\n",
      "|0    |1   |\n",
      "|0    |0   |\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455a8ccf-6f83-467f-b4ef-e49e98c604bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uniform = (\n",
    "    df_uniform\n",
    "    .withColumn(\"salt_values\", F.array([F.lit(i) for i in range(SALT_NUMBER)]))\n",
    "    .withColumn(\"salt\", F.explode(F.col(\"salt_values\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f802a70d-158e-496b-8e9f-b2ce415c5ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----+\n",
      "|value|salt_values|salt|\n",
      "+-----+-----------+----+\n",
      "|0    |[0, 1, 2]  |0   |\n",
      "|0    |[0, 1, 2]  |1   |\n",
      "|0    |[0, 1, 2]  |2   |\n",
      "|1    |[0, 1, 2]  |0   |\n",
      "|1    |[0, 1, 2]  |1   |\n",
      "|1    |[0, 1, 2]  |2   |\n",
      "|2    |[0, 1, 2]  |0   |\n",
      "|2    |[0, 1, 2]  |1   |\n",
      "|2    |[0, 1, 2]  |2   |\n",
      "|3    |[0, 1, 2]  |0   |\n",
      "|3    |[0, 1, 2]  |1   |\n",
      "|3    |[0, 1, 2]  |2   |\n",
      "|4    |[0, 1, 2]  |0   |\n",
      "|4    |[0, 1, 2]  |1   |\n",
      "|4    |[0, 1, 2]  |2   |\n",
      "|5    |[0, 1, 2]  |0   |\n",
      "|5    |[0, 1, 2]  |1   |\n",
      "|5    |[0, 1, 2]  |2   |\n",
      "|6    |[0, 1, 2]  |0   |\n",
      "|6    |[0, 1, 2]  |1   |\n",
      "+-----+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 16:43:20 WARN TaskSetManager: Stage 47 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "df_uniform.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75206f86-10da-4d84-8bdf-fb825cbeaab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_skew.join(df_uniform, [\"value\", \"salt\"], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd237984-e909-48b4-8f45-ffedb6541447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 14:41:50 WARN TaskSetManager: Stage 35 contains a task of very large size (1590 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 37:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+\n",
      "|value|partition| count|\n",
      "+-----+---------+------+\n",
      "|    0|        0|333483|\n",
      "|    0|        1|332851|\n",
      "|    0|        2|333656|\n",
      "|    1|        1|     5|\n",
      "|    2|        0|     2|\n",
      "|    2|        2|     3|\n",
      "+-----+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.withColumn(\"partition\", F.spark_partition_id()).groupBy(\"value\", \"partition\").count().orderBy(\"value\", \"partition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3dec6ed-daf6-4615-be71-efb313cb9a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
